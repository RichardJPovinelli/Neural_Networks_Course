{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RichardJPovinelli/Neural_Networks_Course/blob/main/LSTM_vs_GRU_MicroLab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxmCwUZwwl1l"
      },
      "source": [
        "## Today’s Lab: LSTM vs GRU (Colab‑friendly)\n",
        "\n",
        "**Goal:** Train both models on a tiny synthetic sequence task and compare behavior as the dependency length grows.\n",
        "\n",
        "**How to use this notebook**\n",
        "1. *(Optional)* In Colab: **Runtime → Change runtime type → GPU**. If none is available, stay on CPU.\n",
        "2. Change **ONE** parameter in the **Config** cell (`seq_len`, `gap_k`, `hidden_size`, or `lr`).\n",
        "3. Set seeds to 0/1/2 and **Run all**.\n",
        "4. Fill the small results table and write **two sentences** in the final cell.\n",
        "5. Submit a screenshot of your results (or download this notebook) via the LMS.\n",
        "\n",
        "> If Colab is slow: reduce `hidden_size` (e.g., 32) or `epochs` (e.g., 3). CPU is fine for this task.\n",
        "\n",
        "---\n",
        "\n",
        "*Instructor note:* Once you upload this to GitHub, you can add an “Open in Colab” badge like:\n",
        "\n",
        "`[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_ORG/YOUR_REPO/blob/main/path/to/LSTM_vs_GRU_MicroLab.ipynb)`\n"
      ],
      "id": "fxmCwUZwwl1l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "env-setup",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4dc377-f866-4051-8819-c110a9b8bcf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 | Torch: 2.8.0+cu126 | Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Environment sanity for Colab/local\n",
        "import sys, platform, random, math\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def seed_all(s=0):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(s)\n",
        "\n",
        "seed_all(0)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Python:', sys.version.split()[0], '| Torch:', torch.__version__, '| Device:', device)\n"
      ],
      "id": "env-setup"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da462f4-d9f7-48fa-c5de-325cbce18d3f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'device': 'cpu', 'cell_type': 'lstm', 'seq_len': 30, 'gap_k': 8, 'hidden_size': 64, 'lr': 0.001, 'epochs': 5, 'batch_size': 64, 'seeds': [0, 1, 2]}\n"
          ]
        }
      ],
      "source": [
        "# ---- Config (change ONE for the activity) ----\n",
        "cell_type    = 'lstm'   # 'lstm' or 'gru'\n",
        "seq_len      = 30       # try 10, 30, 60\n",
        "gap_k        = 8        # how far back the label depends on\n",
        "hidden_size  = 64\n",
        "lr           = 1e-3\n",
        "epochs       = 5\n",
        "batch_size   = 64\n",
        "seeds        = [0,1,2]  # activity: run three seeds\n",
        "device       = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print({k:v for k,v in list(locals().items()) if k in ['cell_type','seq_len','gap_k','hidden_size','lr','epochs','batch_size','seeds','device']})\n"
      ],
      "id": "config"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset"
      },
      "execution_count": 3,
      "outputs": [],
      "source": [
        "# ---- Synthetic dataset: binary sequences; label depends on token k steps back ----\n",
        "# Rule: label_t = XOR(current_bit, bit_from_k_steps_ago). Many-to-many labeling.\n",
        "import torch\n",
        "\n",
        "def make_sequence(seq_len, gap_k):\n",
        "    x = torch.randint(0, 2, (seq_len,))\n",
        "    y = torch.zeros(seq_len, dtype=torch.long)\n",
        "    for t in range(seq_len):\n",
        "        prev = x[t-gap_k] if t-gap_k >= 0 else 0\n",
        "        y[t] = int(x[t].item() ^ int(prev))\n",
        "    return x.float(), y\n",
        "\n",
        "class BitSeq(Dataset):\n",
        "    def __init__(self, n_samples, seq_len, gap_k):\n",
        "        self.data = [make_sequence(seq_len, gap_k) for _ in range(n_samples)]\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.data[idx]\n",
        "        return x.unsqueeze(-1), y  # shapes [T,1], [T]\n",
        "\n",
        "def make_dataloaders(seq_len, gap_k, batch_size, seed=0):\n",
        "    seed_all(seed)\n",
        "    train_ds = BitSeq(2000, seq_len, gap_k)\n",
        "    val_ds   = BitSeq(400,  seq_len, gap_k)\n",
        "    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_dl   = DataLoader(val_ds,   batch_size=batch_size)\n",
        "    return train_dl, val_dl\n"
      ],
      "id": "dataset"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "model"
      },
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# ---- Model with LSTM/GRU toggle ----\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, cell='lstm'):\n",
        "        super().__init__()\n",
        "        self.cell_type = cell\n",
        "        if cell == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        elif cell == 'gru':\n",
        "            self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"cell must be 'lstm' or 'gru'\")\n",
        "        self.head = nn.Linear(hidden_size, 2)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)            # [B,T,H]\n",
        "        logits = self.head(out)         # [B,T,2]\n",
        "        return logits\n"
      ],
      "id": "model"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train-utils"
      },
      "execution_count": 5,
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def run_epoch(model, opt, dl, train=True, device='cpu'):\n",
        "    if train: model.train()\n",
        "    else: model.eval()\n",
        "    total, correct, total_loss = 0, 0, 0.0\n",
        "    for xb, yb in dl:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        if train:\n",
        "            opt.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = loss_fn(logits.view(-1, 2), yb.view(-1))\n",
        "        if train:\n",
        "            loss.backward(); opt.step()\n",
        "        total_loss += loss.item() * yb.numel()\n",
        "        preds = logits.argmax(-1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.numel()\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "def train_once(cell_type, seq_len, gap_k, hidden_size, lr, epochs, batch_size, seed, device='cpu'):\n",
        "    train_dl, val_dl = make_dataloaders(seq_len, gap_k, batch_size, seed)\n",
        "    model = SeqModel(hidden_size=hidden_size, cell=cell_type).to(device)\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    tr_hist, va_hist = [], []\n",
        "    for ep in range(1, epochs+1):\n",
        "        tr_loss, tr_acc = run_epoch(model, opt, train_dl, True, device)\n",
        "        va_loss, va_acc = run_epoch(model, opt, val_dl, False, device)\n",
        "        tr_hist.append((tr_loss, tr_acc)); va_hist.append((va_loss, va_acc))\n",
        "        print(f\"ep {ep:02d} | train acc {tr_acc:.3f} | val acc {va_acc:.3f}\")\n",
        "    return va_hist[-1][1]  # final val accuracy\n"
      ],
      "id": "train-utils"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quick-demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da818cb1-d90e-486e-b93c-34518877c25b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demo run with current config ...\n",
            "ep 01 | train acc 0.598 | val acc 0.598\n",
            "ep 02 | train acc 0.629 | val acc 0.630\n",
            "ep 03 | train acc 0.633 | val acc 0.630\n",
            "ep 04 | train acc 0.636 | val acc 0.639\n",
            "ep 05 | train acc 0.659 | val acc 0.680\n",
            "Final val acc (seed 0): 0.680\n"
          ]
        }
      ],
      "source": [
        "# Quick demo (optional): one run to show the loop working\n",
        "print('Demo run with current config ...')\n",
        "acc = train_once(cell_type, seq_len, gap_k, hidden_size, lr, epochs, batch_size, seed=seeds[0], device=device)\n",
        "print(f'Final val acc (seed {seeds[0]}): {acc:.3f}')\n"
      ],
      "id": "quick-demo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "activity",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc4e6181-7c2f-4b9a-c3f1-6f7011ad995c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Activity: run three seeds and record results ===\n",
            "\n",
            "--- LSTM | seed 0 ---\n",
            "ep 01 | train acc 0.598 | val acc 0.598\n",
            "ep 02 | train acc 0.629 | val acc 0.630\n",
            "ep 03 | train acc 0.633 | val acc 0.630\n",
            "ep 04 | train acc 0.636 | val acc 0.639\n",
            "ep 05 | train acc 0.659 | val acc 0.680\n",
            "\n",
            "--- LSTM | seed 1 ---\n",
            "ep 01 | train acc 0.552 | val acc 0.629\n",
            "ep 02 | train acc 0.632 | val acc 0.645\n",
            "ep 03 | train acc 0.633 | val acc 0.645\n",
            "ep 04 | train acc 0.638 | val acc 0.649\n",
            "ep 05 | train acc 0.650 | val acc 0.667\n",
            "\n",
            "--- LSTM | seed 2 ---\n",
            "ep 01 | train acc 0.535 | val acc 0.607\n",
            "ep 02 | train acc 0.623 | val acc 0.636\n",
            "ep 03 | train acc 0.635 | val acc 0.636\n",
            "ep 04 | train acc 0.638 | val acc 0.638\n",
            "ep 05 | train acc 0.658 | val acc 0.672\n",
            "\n",
            "Results table:\n",
            "cell\tseed\tval_acc\n",
            "lstm\t0\t0.680\n",
            "lstm\t1\t0.667\n",
            "lstm\t2\t0.672\n",
            "\n",
            "Saved CSV summary → lstm_gru_activity_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "print('\\n=== Activity: run three seeds and record results ===')\n",
        "results = []\n",
        "for s in seeds:\n",
        "    print(f\"\\n--- {cell_type.upper()} | seed {s} ---\")\n",
        "    acc = train_once(cell_type, seq_len, gap_k, hidden_size, lr, epochs, batch_size, seed=s, device=device)\n",
        "    results.append({'cell': cell_type, 'seed': s, 'val_acc': float(acc)})\n",
        "\n",
        "print('\\nResults table:')\n",
        "print('cell\\tseed\\tval_acc')\n",
        "for r in results:\n",
        "    print(f\"{r['cell']}\\t{r['seed']}\\t{r['val_acc']:.3f}\")\n",
        "\n",
        "# Save CSV summary (works on Colab or local)\n",
        "out_dir = '/content' if 'google.colab' in str(getattr(sys.modules.get('google'), '__name__', '')) else '.'\n",
        "out_path = Path(out_dir)/'lstm_gru_activity_summary.csv'\n",
        "with open(out_path, 'w', newline='') as f:\n",
        "    w = csv.DictWriter(f, fieldnames=['cell','seed','val_acc'])\n",
        "    w.writeheader(); w.writerows(results)\n",
        "print(f\"\\nSaved CSV summary → {out_path}\")\n"
      ],
      "id": "activity"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mnc9QE5Kz0UD"
      },
      "id": "Mnc9QE5Kz0UD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8PLylIM1z0lI"
      },
      "id": "8PLylIM1z0lI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection"
      },
      "source": [
        "### Submit here\n",
        "\n",
        "**Paste your table here and write two sentences:**\n",
        "1) What changed when you modified the one parameter you selected?\n",
        "2) Which cell (LSTM or GRU) handled the change better? Offer a short hypothesis why.\n",
        "\n",
        "*If you finish early (stretch):*\n",
        "- Try `bidirectional=True` in the RNN init and explain why it helps here but is invalid for real-time streaming.\n",
        "- Switch to **many-to-one** prediction by using only `logits[:,-1]` and predicting the final label.\n",
        "- Replace Adam with SGD+momentum and compare convergence speed for the same wall-clock budget.\n"
      ],
      "id": "reflection"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "checks"
      },
      "source": [
        "### Quick checks for understanding\n",
        "1) In an LSTM, which gate primarily controls **exposure** of cell state to the hidden state at time *t*?\n",
        "2) If you double `seq_len` (holding `gap_k` fixed), what failure pattern do you expect in a vanilla RNN vs an LSTM/GRU? Why?\n",
        "3) With `gap_k=15` and `hidden_size=16`, which model (LSTM/GRU) is more likely to maintain >70% accuracy? Defend your pick.\n",
        "4) In the training code, why do we use `logits.view(-1, 2)` and `yb.view(-1)` before computing loss?\n"
      ],
      "id": "checks"
    }
  ]
}