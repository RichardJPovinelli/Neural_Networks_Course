{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b29ecc6",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichardJPovinelli/Neural_Networks_Course/blob/main/Dynamic_MLP_MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "839cd6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import math, time, json, itertools, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e56b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71272b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 55000, Val: 5000, Test: 10000\n"
     ]
    }
   ],
   "source": [
    "# Data Loaders (MNIST)\n",
    "batch_size_default = 128\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "# Split train into train/val\n",
    "val_size = 5000\n",
    "train_size = len(train_ds) - val_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(\n",
    "    train_ds, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "\n",
    "def make_loaders(batch_size):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_ds, batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_ds, batch_size=batch_size, shuffle=False),\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders(batch_size_default)\n",
    "print(f\"Train: {train_size}, Val: {val_size}, Test: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac7bd699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param count test model: 134794\n"
     ]
    }
   ],
   "source": [
    "# Dynamic MLP Model with optional BatchNorm, Dropout, Residual Blocks\n",
    "class DynamicMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=28 * 28,\n",
    "        num_classes=10,\n",
    "        hidden_width=256,\n",
    "        depth=4,\n",
    "        activation=\"relu\",\n",
    "        dropout=0.0,\n",
    "        batchnorm=False,\n",
    "        residual=False,\n",
    "        widths_list=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        act_map = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"tanh\": nn.Tanh(),\n",
    "            \"sigmoid\": nn.Sigmoid(),\n",
    "            \"leakyrelu\": nn.LeakyReLU(0.2),\n",
    "            \"elu\": nn.ELU(),\n",
    "        }\n",
    "        self.activation = act_map[activation.lower()]\n",
    "        self.dropout_p = dropout\n",
    "        self.batchnorm = batchnorm\n",
    "        self.residual = residual\n",
    "        if widths_list is not None and len(widths_list) > 0:\n",
    "            widths = widths_list\n",
    "            depth = len(widths)\n",
    "        else:\n",
    "            widths = [hidden_width] * depth\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        layers = []\n",
    "        prev = input_dim\n",
    "        self.skip_dims = []\n",
    "        for w in widths:\n",
    "            block = []\n",
    "            linear = nn.Linear(prev, w)\n",
    "            block.append(linear)\n",
    "            if batchnorm:\n",
    "                block.append(nn.BatchNorm1d(w))\n",
    "            block.append(self.activation)\n",
    "            if self.dropout_p > 0:\n",
    "                block.append(nn.Dropout(self.dropout_p))\n",
    "            layers.append(nn.Sequential(*block))\n",
    "            self.skip_dims.append((prev, w))\n",
    "            prev = w\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.out = nn.Linear(prev, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = x\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            inp = out\n",
    "            out = layer(out)\n",
    "            if self.residual:\n",
    "                in_dim, out_dim = self.skip_dims[i]\n",
    "                if in_dim == out_dim:\n",
    "                    out = out + inp\n",
    "        return self.out(out)\n",
    "\n",
    "    def count_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Quick sanity test\n",
    "model_test = DynamicMLP(depth=3, hidden_width=128, activation=\"relu\", residual=True)\n",
    "print(\"Param count test model:\", model_test.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38aea678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Utilities\n",
    "EXPERIMENT_LOG: list[dict] = []\n",
    "@dataclass\n",
    "class RunConfig:\n",
    "    depth: int = 4\n",
    "    hidden_width: int = 256\n",
    "    activation: str = 'relu'\n",
    "    dropout: float = 0.0\n",
    "    batchnorm: bool = False\n",
    "    residual: bool = False\n",
    "    optimizer: str = 'adam'\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 128\n",
    "    scheduler: bool = False\n",
    "    step_size: int = 3\n",
    "    gamma: float = 0.5\n",
    "    widths_list: list | None = None\n",
    "    lm_max_iter: int = 20\n",
    "    lm_history_size: int = 10\n",
    "\n",
    "def make_model(cfg: RunConfig):\n",
    "    return DynamicMLP(depth=cfg.depth, hidden_width=cfg.hidden_width, activation=cfg.activation,\n",
    "                       dropout=cfg.dropout, batchnorm=cfg.batchnorm, residual=cfg.residual,\n",
    "                       widths_list=cfg.widths_list).to(device)\n",
    "\n",
    "def make_optimizer(cfg: RunConfig, params):\n",
    "    opt_name = cfg.optimizer.lower()\n",
    "    if cfg.scheduler and opt_name not in ('adamw', 'lm'):\n",
    "        opt_name = 'adamw'\n",
    "    if opt_name == 'lm':\n",
    "        return torch.optim.LBFGS(\n",
    "            params,\n",
    "            lr=cfg.lr,\n",
    "            max_iter=cfg.lm_max_iter,\n",
    "            history_size=cfg.lm_history_size,\n",
    "            line_search_fn='strong_wolfe'\n",
    "        )\n",
    "    if opt_name == 'sgd':\n",
    "        return torch.optim.SGD(params, lr=cfg.lr, momentum=0.9)\n",
    "    if opt_name == 'adamw':\n",
    "        return torch.optim.AdamW(params, lr=cfg.lr)\n",
    "    return torch.optim.Adam(params, lr=cfg.lr)\n",
    "\n",
    "def make_scheduler(cfg: RunConfig, optimizer):\n",
    "    if cfg.scheduler:\n",
    "        if isinstance(optimizer, torch.optim.LBFGS):\n",
    "            print('Scheduler skipped: not supported with LM optimizer.')\n",
    "            return None\n",
    "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.step_size, gamma=cfg.gamma)\n",
    "    return None\n",
    "\n",
    "def epoch_error_correct(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    incorrect = (preds != targets).sum().item()\n",
    "    return incorrect\n",
    "\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    uses_closure = is_train and isinstance(optimizer, torch.optim.LBFGS)\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss, total_incorrect, total_samples = 0.0, 0, 0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        if uses_closure:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                logits_closure = model(xb)\n",
    "                loss_closure = F.cross_entropy(logits_closure, yb)\n",
    "                loss_closure.backward()\n",
    "                return loss_closure\n",
    "            loss = optimizer.step(closure)\n",
    "            loss_value = loss.item() if torch.is_tensor(loss) else float(loss)\n",
    "            with torch.no_grad():\n",
    "                logits = model(xb)\n",
    "        else:\n",
    "            with torch.set_grad_enabled(is_train):\n",
    "                logits = model(xb)\n",
    "                loss = F.cross_entropy(logits, yb)\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            loss_value = loss.item()\n",
    "        total_loss += loss_value * xb.size(0)\n",
    "        total_incorrect += epoch_error_correct(logits, yb)\n",
    "        total_samples += xb.size(0)\n",
    "    avg_loss = total_loss / total_samples\n",
    "    error_rate = total_incorrect / total_samples\n",
    "    return avg_loss, error_rate\n",
    "\n",
    "def summarize_run(cfg: RunConfig, history: dict, test_loss: float, test_error: float) -> dict:\n",
    "    return {\n",
    "        'run_id': len(EXPERIMENT_LOG) + 1,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'optimizer': history.get('optimizer_used', cfg.optimizer),\n",
    "        'scheduler': history.get('scheduler_used', False),\n",
    "        'depth': cfg.depth,\n",
    "        'width': cfg.hidden_width if cfg.widths_list is None else str(cfg.widths_list),\n",
    "        'activation': cfg.activation,\n",
    "        'dropout': cfg.dropout,\n",
    "        'batchnorm': cfg.batchnorm,\n",
    "        'residual': cfg.residual,\n",
    "        'epochs': cfg.epochs,\n",
    "        'batch_size': cfg.batch_size,\n",
    "        'best_val_error': min(history['val_error']) if history['val_error'] else float('nan'),\n",
    "        'test_error': test_error,\n",
    "        'test_loss': test_loss\n",
    "    }\n",
    "\n",
    "def run_experiment(cfg: RunConfig):\n",
    "    train_loader, val_loader, test_loader = make_loaders(cfg.batch_size)\n",
    "    model = make_model(cfg)\n",
    "    optimizer = make_optimizer(cfg, model.parameters())\n",
    "    scheduler = make_scheduler(cfg, optimizer)\n",
    "    history = {'epoch': [], 'train_loss': [], 'train_error': [], 'val_loss': [], 'val_error': []}\n",
    "    history['optimizer_used'] = 'lm' if isinstance(optimizer, torch.optim.LBFGS) else optimizer.__class__.__name__.lower()\n",
    "    history['scheduler_used'] = scheduler is not None\n",
    "    best_val_error = 1.0\n",
    "    best_state = None\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        tl, te = run_epoch(model, train_loader, optimizer)\n",
    "        vl, ve = run_epoch(model, val_loader, None)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(tl)\n",
    "        history['train_error'].append(te)\n",
    "        history['val_loss'].append(vl)\n",
    "        history['val_error'].append(ve)\n",
    "        if ve < best_val_error:\n",
    "            best_val_error = ve\n",
    "            best_state = model.state_dict()\n",
    "        print(f'Epoch {epoch}: TL={tl:.4f} TE={te:.4f} VL={vl:.4f} VE={ve:.4f}')\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    test_loss, test_error = run_epoch(model, test_loader, None)\n",
    "    print(f'Test: loss={test_loss:.4f} error_rate={test_error:.4f}')\n",
    "    history['test_loss'] = test_loss\n",
    "    history['test_error'] = test_error\n",
    "    EXPERIMENT_LOG.append(summarize_run(cfg, history, test_loss, test_error))\n",
    "    return model, history\n",
    "\n",
    "def plot_history(history, title='Training/Validation Error'):\n",
    "    epochs = history['epoch']\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(epochs, history['train_error'], label='Train Error')\n",
    "    plt.plot(epochs, history['val_error'], label='Val Error')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "    print(f\"Test Error: {history['test_error']:.4f}  Test Loss: {history['test_loss']:.4f}\")\n",
    "\n",
    "def plot_run_log(metric: str = 'test_error'):\n",
    "    if not EXPERIMENT_LOG:\n",
    "        print('No experiments logged yet.')\n",
    "        return\n",
    "    df = pd.DataFrame(EXPERIMENT_LOG)\n",
    "    if metric not in df.columns:\n",
    "        print(f\"Metric '{metric}' not found in run log columns: {list(df.columns)}\")\n",
    "        return\n",
    "    metric_title = metric.replace('_', ' ').title()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(df['run_id'], df[metric], color='steelblue')\n",
    "    plt.xlabel('Run ID')\n",
    "    plt.ylabel(metric_title)\n",
    "    plt.title(f'{metric_title} by Run')\n",
    "    plt.xticks(df['run_id'])\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b8db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Widgets (if available)\n",
    "def on_run(b):\n",
    "    run_button.disabled = True\n",
    "    with out_area:\n",
    "        out_area.clear_output(wait=True)\n",
    "        if widths_text.value.strip():\n",
    "            try:\n",
    "                wl = [int(x.strip()) for x in widths_text.value.split(',') if x.strip()]\n",
    "            except ValueError:\n",
    "                print('Invalid widths list; using uniform width.')\n",
    "                wl = None\n",
    "        else:\n",
    "            wl = None\n",
    "        cfg = RunConfig(\n",
    "            depth=depth_w.value,\n",
    "            hidden_width=width_w.value,\n",
    "            activation=act_w.value,\n",
    "            dropout=dropout_w.value,\n",
    "            batchnorm=batchnorm_w.value,\n",
    "            residual=residual_w.value,\n",
    "            optimizer=optimizer_w.value,\n",
    "            lr=lr_w.value,\n",
    "            epochs=epochs_w.value,\n",
    "            batch_size=batch_w.value,\n",
    "            scheduler=scheduler_w.value,\n",
    "            step_size=step_size_w.value,\n",
    "            gamma=gamma_w.value,\n",
    "            widths_list=wl,\n",
    "            lm_max_iter=lm_iter_w.value,\n",
    "            lm_history_size=lm_hist_w.value,\n",
    "        )\n",
    "        if cfg.optimizer.lower() == 'lm' and cfg.scheduler:\n",
    "            print('Scheduler request ignored: LM optimizer does not support StepLR.')\n",
    "        print('Running with config:', cfg)\n",
    "        model, history = run_experiment(cfg)\n",
    "        plt.close('all')\n",
    "        opt_label = history.get('optimizer_used', cfg.optimizer)\n",
    "        plot_history(history, title=f'Error Curves (opt={opt_label}, depth={cfg.depth})')\n",
    "        plot_run_log(metric_dropdown.value)\n",
    "    run_button.disabled = False\n",
    "\n",
    "if WIDGETS_AVAILABLE:\n",
    "    depth_w = widgets.IntSlider(description='Depth', min=1, max=12, value=4)\n",
    "    width_w = widgets.IntSlider(description='Width', min=32, max=1024, step=32, value=256)\n",
    "    act_w = widgets.Dropdown(description='Activation', options=['relu','gelu','tanh','sigmoid','leakyrelu','elu'], value='relu')\n",
    "    dropout_w = widgets.FloatSlider(description='Dropout', min=0.0, max=0.7, step=0.05, value=0.0)\n",
    "    batchnorm_w = widgets.ToggleButton(description='BatchNorm', value=False)\n",
    "    residual_w = widgets.ToggleButton(description='Residual', value=False)\n",
    "    optimizer_w = widgets.Dropdown(description='Optimizer', options=['adam','adamw','sgd','lm'], value='adam')\n",
    "    lr_w = widgets.FloatLogSlider(description='LR', base=10, min=-4, max=-1, step=0.1, value=1e-3)\n",
    "    epochs_w = widgets.IntSlider(description='Epochs', min=1, max=20, value=5)\n",
    "    batch_w = widgets.IntSlider(description='Batch', min=32, max=256, step=32, value=128)\n",
    "    scheduler_w = widgets.ToggleButton(description='Scheduler', value=False)\n",
    "    step_size_w = widgets.IntSlider(description='StepSize', min=1, max=10, value=3)\n",
    "    gamma_w = widgets.FloatSlider(description='Gamma', min=0.1, max=0.9, step=0.05, value=0.5)\n",
    "    lm_iter_w = widgets.IntSlider(description='LM max_iter', min=1, max=50, value=20)\n",
    "    lm_hist_w = widgets.IntSlider(description='LM history', min=1, max=50, value=10)\n",
    "    widths_text = widgets.Text(description='WidthsList', placeholder='e.g. 128,256,256')\n",
    "    metric_dropdown = widgets.Dropdown(description='Metric', options=['test_error','best_val_error','test_loss'], value='test_error')\n",
    "    run_button = widgets.Button(description='Run Experiment', button_style='success')\n",
    "    out_area = widgets.Output()\n",
    "    run_button.on_click(on_run, remove=True)\n",
    "    run_button.on_click(on_run)\n",
    "    display(\n",
    "        widgets.VBox([\n",
    "            widgets.HBox([depth_w, width_w, act_w, dropout_w]),\n",
    "            widgets.HBox([batchnorm_w, residual_w, optimizer_w, lr_w]),\n",
    "            widgets.HBox([epochs_w, batch_w, scheduler_w, step_size_w, gamma_w]),\n",
    "            widgets.HBox([lm_iter_w, lm_hist_w, metric_dropdown]),\n",
    "            widths_text,\n",
    "            run_button,\n",
    "            out_area,\n",
    "        ])\n",
    ")\n",
    "else:\n",
    "    print('ipywidgets not available. Use manual config in next cell.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc1a82",
   "metadata": {},
   "source": [
    "### Run Configuration Parameters\n",
    "- `depth`: number of hidden layers (ignored when `widths_list` is set; default 4).\n",
    "- `hidden_width`: neurons per hidden layer when using uniform widths (default 256).\n",
    "- `activation`: activation function name (`relu`, `gelu`, `tanh`, `sigmoid`, `leakyrelu`, `elu`).\n",
    "- `dropout`: dropout probability applied after activations (0 disables).\n",
    "- `batchnorm`: enable per-layer batch normalization (`True`/`False`).\n",
    "- `residual`: add skip connections when layer dimensions align (`True`/`False`).\n",
    "- `optimizer`: optimizer choice (`adam`, `adamw`, `sgd`, `lm`; `lm` uses an LBFGS-based Levenberg–Marquardt-like step and ignores schedulers).\n",
    "- `lr`: learning rate supplied to the optimizer (default 1e-3).\n",
    "- `epochs`: total training epochs (default 5).\n",
    "- `batch_size`: mini-batch size used by data loaders (default 128).\n",
    "- `scheduler`: toggle StepLR scheduler usage (`True`/`False`).\n",
    "- `step_size`: StepLR step interval in epochs when scheduler is active (default 3).\n",
    "- `widths_list`: optional explicit sequence of layer widths that overrides `hidden_width`/`depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7301a7fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d795175314f4505b6c13e8c33866c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=4, description='Depth', max=12, min=1), IntSlider(value=256, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if WIDGETS_AVAILABLE:\n",
    "    run_button.on_click(on_run)\n",
    "    display(\n",
    "        widgets.VBox([\n",
    "            widgets.HBox([depth_w, width_w, act_w, dropout_w]),\n",
    "            widgets.HBox([batchnorm_w, residual_w, optimizer_w, lr_w]),\n",
    "            widgets.HBox([epochs_w, batch_w, scheduler_w, step_size_w, gamma_w]),\n",
    "            widgets.HBox([lm_iter_w, lm_hist_w, metric_dropdown]),\n",
    "            widths_text,\n",
    "            run_button,\n",
    "            out_area,\n",
    "        ])\n",
    "    )\n",
    "else:\n",
    "    print('ipywidgets not available. Use manual config in next cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550821f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching example configuration: RunConfig(depth=4, hidden_width=256, activation='relu', dropout=0.1, batchnorm=True, residual=True, optimizer='adam', lr=0.001, epochs=5, batch_size=128, scheduler=False, step_size=3, gamma=0.5, widths_list=None, lm_max_iter=20, lm_history_size=10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      2\u001b[39m example_cfg = RunConfig(\n\u001b[32m      3\u001b[39m     depth=\u001b[32m4\u001b[39m,\n\u001b[32m      4\u001b[39m     hidden_width=\u001b[32m256\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLaunching example configuration:\u001b[39m\u001b[33m'\u001b[39m, example_cfg)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m example_model, example_history = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m plot_history(\n\u001b[32m     18\u001b[39m     example_history,\n\u001b[32m     19\u001b[39m     title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExample Run (activation=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_cfg.activation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_cfg.depth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, width=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_cfg.hidden_width\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     21\u001b[39m plot_run_log(\u001b[33m'\u001b[39m\u001b[33mtest_error\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    123\u001b[39m best_state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, cfg.epochs + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     tl, te = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     vl, ve = run_epoch(model, val_loader, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, loader, optimizer)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_train:\n\u001b[32m     84\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     optimizer.step()\n\u001b[32m     87\u001b[39m loss_value = loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richard.povinelli\\OneDrive - Marquette University\\mu\\teaching\\Neural Networks\\lectures\\jupyter_notebooks\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richard.povinelli\\OneDrive - Marquette University\\mu\\teaching\\Neural Networks\\lectures\\jupyter_notebooks\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\richard.povinelli\\OneDrive - Marquette University\\mu\\teaching\\Neural Networks\\lectures\\jupyter_notebooks\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example training run on MNIST\n",
    "example_cfg = RunConfig(\n",
    "    depth=4,\n",
    "    hidden_width=256,\n",
    "    activation='relu',\n",
    "    dropout=0.1,\n",
    "    batchnorm=True,\n",
    "    residual=True,\n",
    "    optimizer='adam',\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    scheduler=False\n",
    ")\n",
    "# print('Launching example configuration:', example_cfg)\n",
    "#example_model, example_history = run_experiment(example_cfg)\n",
    "# plot_history(\n",
    "#     example_history,\n",
    "#     title=f\"Example Run (activation={example_cfg.activation}, depth={example_cfg.depth}, width={example_cfg.hidden_width})\"\n",
    "# )\n",
    "# plot_run_log('test_error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
