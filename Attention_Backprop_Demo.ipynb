{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7a7a35",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichardJPovinelli/Neural_Networks_Course/blob/main/Attention_Backprop_Demo.ipynb)\n",
    "# Backpropagation Through Scaled Dot-Product Attention (Demo)\n",
    "\n",
    "This notebook demonstrates **backpropagation through a tiny self-attention block**.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Define a toy self-attention module (single head) with small, fixed matrices.\n",
    "2. Run a forward pass and define a simple scalar loss.\n",
    "3. Use PyTorch autograd to compute gradients.\n",
    "4. Manually compute the same gradients using the analytical formulas for attention.\n",
    "5. Compare **autograd vs. manual vs. finite-difference** to verify everything matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "PRECISION = 2\n",
    "\n",
    "torch.set_printoptions(precision=PRECISION, sci_mode=True)\n",
    "# choose best available device automatically\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon GPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63dbac",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Tiny Self-Attention Setup\n",
    "\n",
    "We use:\n",
    "- Sequence length $n = 3$\n",
    "- Model dimension $d_{model} = d_k = d_v = 4$\n",
    "- Single-head attention\n",
    "- Fixed numerical values so every run is reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd1ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "tensor([[ 5.000e-01,  2.000e-01,  1.000e-01, -1.000e-01],\n",
      "        [ 0.000e+00,  3.000e-01, -2.000e-01,  2.000e-01],\n",
      "        [ 4.000e-01, -1.000e-01,  0.000e+00,  3.000e-01]])\n",
      "W_Q.shape = torch.Size([4, 4])\n",
      "W_K.shape = torch.Size([4, 4])\n",
      "W_V.shape = torch.Size([4, 4])\n",
      "T.shape = torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fixed toy inputs (3 tokens, 4-dim embeddings)\n",
    "X = torch.tensor([\n",
    "    [0.5, 0.2, 0.1, -0.1],\n",
    "    [0.0, 0.3, -0.2, 0.2],\n",
    "    [0.4, -0.1, 0.0, 0.3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Fixed projection matrices\n",
    "W_Q = torch.tensor([\n",
    "    [0.2, -0.1, 0.0, 0.3],\n",
    "    [0.1,  0.0, 0.2, -0.2],\n",
    "    [-0.1, 0.3, 0.1, 0.0],\n",
    "    [0.0,  0.2, -0.2, 0.1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "W_K = torch.tensor([\n",
    "    [0.1,  0.2, 0.0, -0.1],\n",
    "    [0.0,  0.1, 0.3,  0.0],\n",
    "    [0.2, -0.2, 0.1,  0.1],\n",
    "    [-0.1, 0.0, 0.2,  0.2]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "W_V = torch.tensor([\n",
    "    [0.3,  0.1, 0.0, -0.2],\n",
    "    [0.0,  0.2, 0.1,  0.0],\n",
    "    [0.1, -0.1, 0.2,  0.1],\n",
    "    [0.0,  0.1, -0.2, 0.3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "d_k = 4\n",
    "scale = 1.0 / math.sqrt(d_k)\n",
    "\n",
    "# Simple target for the loss: same shape as A\n",
    "T = torch.tensor([\n",
    "    [0.10, 0.00, 0.05, -0.05],\n",
    "    [0.00, 0.10, -0.05, 0.05],\n",
    "    [0.05, -0.05, 0.10, 0.00]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"X = \\n{X}\\nW_Q.shape = {W_Q.shape}\\nW_K.shape = {W_K.shape}\\nW_V.shape = {W_V.shape}\\nT.shape = {T.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fb170",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Forward Pass\n",
    "\n",
    "We implement scaled dot-product self-attention:\n",
    "\n",
    "\n",
    "$Q = X W_Q, \\quad$\n",
    "$K = X W_K, \\quad$\n",
    "$V = X W_V$\n",
    "\n",
    "\n",
    "$S = \\frac{QK^\\top}{\\sqrt{d_k}}, \\quad$\n",
    "$P = \\text{softmax}(S) \\text{ (row-wise)}, \\quad$\n",
    "$A = P V$\n",
    "\n",
    "Loss:\n",
    "$\\mathcal{L} = \\tfrac12 \\|A - T\\|^2.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ddaa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output A:\n",
      "tensor([[ 8.670e-02,  7.332e-02, -1.999e-02, -2.335e-02],\n",
      "        [ 8.680e-02,  7.331e-02, -1.990e-02, -2.347e-02],\n",
      "        [ 8.675e-02,  7.329e-02, -2.007e-02, -2.330e-02]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "\n",
      "Loss L: 2.861e-02\n"
     ]
    }
   ],
   "source": [
    "# Clone parameters with requires_grad for autograd\n",
    "X_ag  = X.clone().detach().requires_grad_(True)\n",
    "WQ_ag = W_Q.clone().detach().requires_grad_(True)\n",
    "WK_ag = W_K.clone().detach().requires_grad_(True)\n",
    "WV_ag = W_V.clone().detach().requires_grad_(True)\n",
    "\n",
    "def attention_forward(X_, WQ_, WK_, WV_):\n",
    "    Q = X_ @ WQ_\n",
    "    K = X_ @ WK_\n",
    "    V = X_ @ WV_\n",
    "    S = (Q @ K.transpose(0, 1)) * scale\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    A = P @ V\n",
    "    return Q, K, V, S, P, A\n",
    "\n",
    "Q_ag, K_ag, V_ag, S_ag, P_ag, A_ag = attention_forward(X_ag, WQ_ag, WK_ag, WV_ag)\n",
    "L_ag = 0.5 * torch.sum((A_ag - T)**2)\n",
    "\n",
    "print(\"Attention output A:\")\n",
    "print(A_ag)\n",
    "print(f\"\\nLoss L: {L_ag:0.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cde83",
   "metadata": {},
   "source": [
    "## 2. Backward Pass (with Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc56790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autograd gradient dL/dW_Q:\n",
      "tensor([[-1.272e-04, -3.362e-05,  3.323e-05,  8.967e-05],\n",
      "        [ 7.927e-05,  1.711e-05, -3.356e-05, -4.563e-05],\n",
      "        [-9.281e-05, -2.068e-05,  3.714e-05,  5.514e-05],\n",
      "        [ 6.987e-05,  1.308e-05, -3.626e-05, -3.487e-05]])\n",
      "\n",
      "Autograd gradient dL/dW_K:\n",
      "tensor([[-1.752e-05, -4.336e-06, -2.968e-05, -1.466e-04],\n",
      "        [-2.176e-05,  1.324e-06,  1.287e-05, -1.607e-05],\n",
      "        [-1.367e-05, -2.544e-06, -1.694e-05, -9.365e-05],\n",
      "        [ 3.736e-05,  2.116e-06,  1.044e-05,  1.362e-04]])\n",
      "\n",
      "Autograd gradient dL/dW_V:\n",
      "tensor([[ 3.312e-02,  5.101e-02, -4.801e-02, -2.107e-02],\n",
      "        [ 1.468e-02,  2.260e-02, -2.127e-02, -9.339e-03],\n",
      "        [-3.652e-03, -5.654e-03,  5.323e-03,  2.319e-03],\n",
      "        [ 1.469e-02,  2.269e-02, -2.137e-02, -9.335e-03]])\n"
     ]
    }
   ],
   "source": [
    "# Retain gradients for intermediate (non-leaf) tensors so their .grad fields are populated\n",
    "# when we call backward(). This makes it easy to inspect Q_ag.grad, K_ag.grad, etc.\n",
    "for t in (Q_ag, K_ag, V_ag, S_ag, P_ag, A_ag):\n",
    "    # Only call retain_grad() on tensors that require grad and are not leafs\n",
    "    if isinstance(t, torch.Tensor) and t.requires_grad and not t.is_leaf:\n",
    "        t.retain_grad()\n",
    "\n",
    "L_ag.backward()\n",
    "\n",
    "def safe_clone_grad(tensor):\n",
    "    if tensor is None:\n",
    "        return None\n",
    "    g = tensor.grad\n",
    "    if g is None:\n",
    "        return None\n",
    "    return g.detach().clone()\n",
    "\n",
    "grads_auto = {\n",
    "    \"X\": safe_clone_grad(X_ag),\n",
    "    \"W_Q\": safe_clone_grad(WQ_ag),\n",
    "    \"W_K\": safe_clone_grad(WK_ag),\n",
    "    \"W_V\": safe_clone_grad(WV_ag),\n",
    "    \"Q\": safe_clone_grad(Q_ag),\n",
    "    \"K\": safe_clone_grad(K_ag),\n",
    "    \"V\": safe_clone_grad(V_ag),\n",
    "    \"S\": safe_clone_grad(S_ag),\n",
    "    \"P\": safe_clone_grad(P_ag),\n",
    "    \"A\": safe_clone_grad(A_ag),\n",
    "}\n",
    "\n",
    "print(\"\\nAutograd gradient dL/dW_Q:\")\n",
    "print(grads_auto[\"W_Q\"])\n",
    "print(\"\\nAutograd gradient dL/dW_K:\")\n",
    "print(grads_auto[\"W_K\"])\n",
    "print(\"\\nAutograd gradient dL/dW_V:\")\n",
    "print(grads_auto[\"W_V\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c161d4",
   "metadata": {},
   "source": [
    "## 3. Manual Backpropagation\n",
    "\n",
    "We recompute the forward pass without autograd and apply the analytical formulas.\n",
    "\n",
    "Start with:\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial A} = A - T$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Through $A = P V$:\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial V} = P^\\top \\frac{\\partial \\mathcal{L}}{\\partial A}, \\quad$\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial P} = \\frac{\\partial \\mathcal{L}}{\\partial A} V^\\top$\n",
    "\n",
    "2. Softmax (row-wise for each query $i$):\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s_i} = J_{\\text{softmax}}(p_i)^{\\top}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p_i},\n",
    "\\quad\n",
    "J_{\\text{softmax}}(p_i) = \\mathrm{diag}(p_i) - p_i p_i^\\top\n",
    "$\n",
    "\n",
    "3. Through $S = QK^\\top / \\sqrt{d_k}$:\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Q}\n",
    "= \\frac{1}{\\sqrt{d_k}} \\frac{\\partial \\mathcal{L}}{\\partial S} K,\n",
    "\\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial K}\n",
    "= \\frac{1}{\\sqrt{d_k}}\n",
    "\\left(\\frac{\\partial \\mathcal{L}}{\\partial S}\\right)^{\\top} Q\n",
    "$\n",
    "\n",
    "4. Through projections:\n",
    "   \n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_Q} = X^\\top \\frac{\\partial \\mathcal{L}}{\\partial Q},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_K} = X^\\top \\frac{\\partial \\mathcal{L}}{\\partial K},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_V} = X^\\top \\frac{\\partial \\mathcal{L}}{\\partial V}\n",
    "$\n",
    "\n",
    "$ \\frac{\\partial \\mathcal{L}}{\\partial X} = \\frac{\\partial \\mathcal{L}}{\\partial Q} W_Q^\\top + \\frac{\\partial \\mathcal{L}}{\\partial K} W_K^\\top + \\frac{\\partial \\mathcal{L}}{\\partial V} W_V^\\top $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d65f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from manual forward: 2.8612e-02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward without autograd\n",
    "with torch.no_grad():\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    S = (Q @ K.t()) * scale\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    A = P @ V\n",
    "    L = 0.5 * torch.sum((A - T)**2)\n",
    "\n",
    "# 1) dL/dA\n",
    "dA = (A - T)\n",
    "\n",
    "# 2) Through A = P V\n",
    "dV = P.t() @ dA\n",
    "dP = dA @ V.t()\n",
    "\n",
    "# 3) Softmax Jacobian row-wise\n",
    "def softmax_grad_row(p_row, dp_row):\n",
    "    # p_row, dp_row: 1D tensors of length n\n",
    "    p = p_row.view(-1, 1)\n",
    "    J = torch.diagflat(p) - p @ p.t()\n",
    "    return (J.t() @ dp_row.view(-1, 1)).view(-1)\n",
    "\n",
    "n = P.shape[0]\n",
    "dS = torch.zeros_like(S)\n",
    "for i in range(n):\n",
    "    dS[i] = softmax_grad_row(P[i], dP[i])\n",
    "\n",
    "# 4) Through S = (QK^T)/sqrt(d_k)\n",
    "dQ = (dS @ K) * scale\n",
    "dK = (dS.t() @ Q) * scale\n",
    "\n",
    "# 5) Through projections\n",
    "dW_Q = X.t() @ dQ\n",
    "dW_K = X.t() @ dK\n",
    "dW_V = X.t() @ dV\n",
    "\n",
    "dX = dQ @ W_Q.t() + dK @ W_K.t() + dV @ W_V.t()\n",
    "\n",
    "grads_manual = {\n",
    "    \"X\": dX,\n",
    "    \"W_Q\": dW_Q,\n",
    "    \"W_K\": dW_K,\n",
    "    \"W_V\": dW_V,\n",
    "    \"Q\": dQ,\n",
    "    \"K\": dK,\n",
    "    \"V\": dV,\n",
    "    \"S\": dS,\n",
    "    \"P\": dP,\n",
    "    \"A\": dA,\n",
    "}\n",
    "\n",
    "print(f\"Loss from manual forward: {L:.{PRECISION}e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb1f6d",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Compare Manual vs Autograd Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512c1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max |manual - autograd| for W_Q: 2.546585e-11\n",
      "Max |manual - autograd| for W_K: 2.910383e-11\n",
      "Max |manual - autograd| for W_V: 0.000000e+00\n",
      "Max |manual - autograd| for X: 1.862645e-09\n",
      "\n",
      "Autograd dL/dW_Q:\n",
      " tensor([[-1.272e-04, -3.362e-05,  3.323e-05,  8.967e-05],\n",
      "        [ 7.927e-05,  1.711e-05, -3.356e-05, -4.563e-05],\n",
      "        [-9.281e-05, -2.068e-05,  3.714e-05,  5.514e-05],\n",
      "        [ 6.987e-05,  1.308e-05, -3.626e-05, -3.487e-05]])\n",
      "\n",
      "Manual   dL/dW_Q:\n",
      " tensor([[-1.272e-04, -3.362e-05,  3.323e-05,  8.967e-05],\n",
      "        [ 7.927e-05,  1.711e-05, -3.356e-05, -4.563e-05],\n",
      "        [-9.281e-05, -2.068e-05,  3.714e-05,  5.514e-05],\n",
      "        [ 6.987e-05,  1.308e-05, -3.626e-05, -3.487e-05]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return float(torch.max(torch.abs(a - b)))\n",
    "\n",
    "for name in [\"W_Q\", \"W_K\", \"W_V\", \"X\"]:\n",
    "    diff = max_abs_diff(grads_manual[name], grads_auto[name])\n",
    "    print(f\"Max |manual - autograd| for {name}: {diff:{PRECISION}.e}\")\n",
    "\n",
    "print(\"\\nAutograd dL/dW_Q:\\n\", grads_auto[\"W_Q\"])\n",
    "print(\"\\nManual   dL/dW_Q:\\n\", grads_manual[\"W_Q\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc11036",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Finite-Difference Check\n",
    "\n",
    "Pick one element of $W_Q$ and approximate its gradient numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67387633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite-diff grad for W_Q[0,0]: -1.397e-04\n",
      "Autograd grad: -1.272e-04\n",
      "Manual grad: -1.272e-04\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_with_WQ(WQ_new):\n",
    "    Q = X @ WQ_new\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    S = (Q @ K.t()) * scale\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    A = P @ V\n",
    "    return 0.5 * torch.sum((A - T)**2)\n",
    "\n",
    "i, j = 0, 0  # test this entry\n",
    "eps = 1e-4\n",
    "\n",
    "with torch.no_grad():\n",
    "    WQ_plus = W_Q.clone();  WQ_plus[i, j] += eps\n",
    "    WQ_minus = W_Q.clone(); WQ_minus[i, j] -= eps\n",
    "    L_plus = compute_loss_with_WQ(WQ_plus)\n",
    "    L_minus = compute_loss_with_WQ(WQ_minus)\n",
    "    num_grad = (L_plus - L_minus) / (2 * eps)\n",
    "\n",
    "print(f\"Finite-diff grad for W_Q[{i},{j}]: {float(num_grad):.2e}\")\n",
    "print(f\"Autograd grad: {float(grads_auto['W_Q'][i,j]):.2e}\")\n",
    "print(f\"Manual grad: {float(grads_manual['W_Q'][i,j]):.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d166e8d",
   "metadata": {},
   "source": [
    "\n",
    "## 6. What to Notice\n",
    "\n",
    "- Manual gradients match autograd (up to numerical precision).\n",
    "- Each query position's loss sends signal to **all** keys/values it attends to.\n",
    "- The softmax Jacobian step is where this coupling happens.\n",
    "- The $1/\\sqrt{d_k}$ factor keeps scores and gradients in a stable range.\n",
    "\n",
    "## 7. Do the following\n",
    "- Change the target $T$ or matrices and re-run cells.\n",
    "- Predict qualitatively how attention weights and gradients will shift.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
