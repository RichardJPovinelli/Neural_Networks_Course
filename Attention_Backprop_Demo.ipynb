{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7a7a35",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichardJPovinelli/Neural_Networks_Course/blob/main/Attention_Backprop_Demo.ipynb)\n",
    "# Backpropagation Through Scaled Dot-Product Attention (Demo)\n",
    "\n",
    "This notebook demonstrates **backpropagation through a tiny self-attention block**.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. Define a toy self-attention module (single head) with small, fixed matrices.\n",
    "2. Run a forward pass and define a simple scalar loss.\n",
    "3. Use PyTorch autograd to compute gradients.\n",
    "4. Manually compute the same gradients using the analytical formulas for attention.\n",
    "5. Compare **autograd vs. manual vs. finite-difference** to verify everything matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870b80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "PRECISION = 2\n",
    "\n",
    "torch.set_printoptions(precision=PRECISION, sci_mode=True)\n",
    "# choose best available device automatically\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon GPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63dbac",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Tiny Self-Attention Setup\n",
    "\n",
    "We use:\n",
    "- Sequence length $n = 3$\n",
    "- Model dimension $d_{model} = d_k = d_v = 4$\n",
    "- Single-head attention\n",
    "- Fixed numerical values so every run is reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fd1ab67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "tensor([[ 5.00e-01,  2.00e-01,  1.00e-01, -1.00e-01],\n",
      "        [ 0.00e+00,  3.00e-01, -2.00e-01,  2.00e-01],\n",
      "        [ 4.00e-01, -1.00e-01,  0.00e+00,  3.00e-01]])\n",
      "W_Q.shape = torch.Size([4, 4])\n",
      "W_K.shape = torch.Size([4, 4])\n",
      "W_V.shape = torch.Size([4, 4])\n",
      "T.shape = torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fixed toy inputs (3 tokens, 4-dim embeddings)\n",
    "X = torch.tensor([\n",
    "    [0.5, 0.2, 0.1, -0.1],\n",
    "    [0.0, 0.3, -0.2, 0.2],\n",
    "    [0.4, -0.1, 0.0, 0.3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Fixed projection matrices\n",
    "W_Q = torch.tensor([\n",
    "    [0.2, -0.1, 0.0, 0.3],\n",
    "    [0.1,  0.0, 0.2, -0.2],\n",
    "    [-0.1, 0.3, 0.1, 0.0],\n",
    "    [0.0,  0.2, -0.2, 0.1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "W_K = torch.tensor([\n",
    "    [0.1,  0.2, 0.0, -0.1],\n",
    "    [0.0,  0.1, 0.3,  0.0],\n",
    "    [0.2, -0.2, 0.1,  0.1],\n",
    "    [-0.1, 0.0, 0.2,  0.2]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "W_V = torch.tensor([\n",
    "    [0.3,  0.1, 0.0, -0.2],\n",
    "    [0.0,  0.2, 0.1,  0.0],\n",
    "    [0.1, -0.1, 0.2,  0.1],\n",
    "    [0.0,  0.1, -0.2, 0.3]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "d_k = 4\n",
    "\n",
    "# Simple target for the loss: same shape as A\n",
    "T = torch.tensor([\n",
    "    [0.10, 0.00, 0.05, -0.05],\n",
    "    [0.00, 0.10, -0.05, 0.05],\n",
    "    [0.05, -0.05, 0.10, 0.00]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(f\"X = \\n{X}\\nW_Q.shape = {W_Q.shape}\\nW_K.shape = {W_K.shape}\\nW_V.shape = {W_V.shape}\\nT.shape = {T.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e1723a",
   "metadata": {},
   "source": [
    "Turn off scaling by setting SCALE to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE = True #set to False to turn off scaling\n",
    "scale = 1.0 / math.sqrt(d_k) if SCALE else 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fb170",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Forward Pass\n",
    "\n",
    "We implement scaled dot-product self-attention:\n",
    "\n",
    "\n",
    "$Q = X W_Q, \\quad$\n",
    "$K = X W_K, \\quad$\n",
    "$V = X W_V$\n",
    "\n",
    "\n",
    "$S = \\frac{QK^\\top}{\\sqrt{d_k}}, \\quad$\n",
    "$P = \\text{softmax}(S) \\text{ (row-wise)}, \\quad$\n",
    "$A = P V$\n",
    "\n",
    "Loss:\n",
    "$\\mathcal{L} = \\tfrac12 \\|A - T\\|^2.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58ddaa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output A:\n",
      "tensor([[ 8.67e-02,  7.33e-02, -2.00e-02, -2.34e-02],\n",
      "        [ 8.69e-02,  7.33e-02, -1.98e-02, -2.36e-02],\n",
      "        [ 8.68e-02,  7.32e-02, -2.01e-02, -2.33e-02]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Loss L: 2.86e-02\n"
     ]
    }
   ],
   "source": [
    "# Clone parameters with requires_grad for autograd\n",
    "X_autogradient  = X.clone().detach().requires_grad_(True)\n",
    "WQ_autogradient = W_Q.clone().detach().requires_grad_(True)\n",
    "WK_autogradient = W_K.clone().detach().requires_grad_(True)\n",
    "WV_autogradient = W_V.clone().detach().requires_grad_(True)\n",
    "\n",
    "def attention_forward(X_, WQ_, WK_, WV_):\n",
    "    Q = X_ @ WQ_\n",
    "    K = X_ @ WK_\n",
    "    V = X_ @ WV_\n",
    "    S = (Q @ K.transpose(0, 1)) * scale\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    A = P @ V\n",
    "    return Q, K, V, S, P, A\n",
    "\n",
    "Q_autogradient, K_autogradient, V_autogradient, S_autogradient, P_autogradient, A_autogradient = attention_forward(X_autogradient, WQ_autogradient, WK_autogradient, WV_autogradient)\n",
    "L_autogradient = 0.5 * torch.sum((A_autogradient - T)**2)\n",
    "\n",
    "print(\"Attention output A:\")\n",
    "print(A_autogradient)\n",
    "print(f\"\\nLoss L: {L_autogradient:.{PRECISION}e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75cde83",
   "metadata": {},
   "source": [
    "## 3. Backward Pass (with Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fc56790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autograd gradient dL/dW_Q:\n",
      "tensor([[-2.54e-04, -6.72e-05,  6.62e-05,  1.79e-04],\n",
      "        [ 1.59e-04,  3.44e-05, -6.72e-05, -9.17e-05],\n",
      "        [-1.86e-04, -4.14e-05,  7.43e-05,  1.11e-04],\n",
      "        [ 1.40e-04,  2.62e-05, -7.27e-05, -7.00e-05]])\n",
      "\n",
      "Autograd gradient dL/dW_K:\n",
      "tensor([[-3.46e-05, -8.76e-06, -5.94e-05, -2.93e-04],\n",
      "        [-4.36e-05,  2.58e-06,  2.59e-05, -3.27e-05],\n",
      "        [-2.71e-05, -5.15e-06, -3.39e-05, -1.87e-04],\n",
      "        [ 7.44e-05,  4.38e-06,  2.08e-05,  2.73e-04]])\n",
      "\n",
      "Autograd gradient dL/dW_V:\n",
      "tensor([[ 3.32e-02,  5.10e-02, -4.80e-02, -2.11e-02],\n",
      "        [ 1.47e-02,  2.25e-02, -2.12e-02, -9.34e-03],\n",
      "        [-3.64e-03, -5.64e-03,  5.31e-03,  2.31e-03],\n",
      "        [ 1.47e-02,  2.27e-02, -2.14e-02, -9.34e-03]])\n"
     ]
    }
   ],
   "source": [
    "# Retain gradients for intermediate (non-leaf) tensors so their .grad fields are populated\n",
    "# when we call backward(). This makes it easy to inspect Q_ag.grad, K_ag.grad, etc.\n",
    "for t in (Q_autogradient, K_autogradient, V_autogradient, S_autogradient, P_autogradient, A_autogradient):\n",
    "    # Only call retain_grad() on tensors that require grad and are not leafs\n",
    "    if isinstance(t, torch.Tensor) and t.requires_grad and not t.is_leaf:\n",
    "        t.retain_grad()\n",
    "\n",
    "L_autogradient.backward()\n",
    "\n",
    "def safe_clone_grad(tensor):\n",
    "    if tensor is None:\n",
    "        return None\n",
    "    g = tensor.grad\n",
    "    if g is None:\n",
    "        return None\n",
    "    return g.detach().clone()\n",
    "\n",
    "grads_auto = {\n",
    "    \"X\": safe_clone_grad(X_autogradient),\n",
    "    \"W_Q\": safe_clone_grad(WQ_autogradient),\n",
    "    \"W_K\": safe_clone_grad(WK_autogradient),\n",
    "    \"W_V\": safe_clone_grad(WV_autogradient),\n",
    "    \"Q\": safe_clone_grad(Q_autogradient),\n",
    "    \"K\": safe_clone_grad(K_autogradient),\n",
    "    \"V\": safe_clone_grad(V_autogradient),\n",
    "    \"S\": safe_clone_grad(S_autogradient),\n",
    "    \"P\": safe_clone_grad(P_autogradient),\n",
    "    \"A\": safe_clone_grad(A_autogradient),\n",
    "}\n",
    "\n",
    "print(\"\\nAutograd gradient dL/dW_Q:\")\n",
    "print(grads_auto[\"W_Q\"])\n",
    "print(\"\\nAutograd gradient dL/dW_K:\")\n",
    "print(grads_auto[\"W_K\"])\n",
    "print(\"\\nAutograd gradient dL/dW_V:\")\n",
    "print(grads_auto[\"W_V\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c161d4",
   "metadata": {},
   "source": [
    "## 4. Manual Backpropagation\n",
    "\n",
    "We recompute the forward pass without autograd and apply the analytical formulas.\n",
    "\n",
    "Start with:\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial A} = A - T$\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Through $A = P V$:\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial V} = P^\\top \\frac{\\partial \\mathcal{L}}{\\partial A}, \\quad$\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial P} = \\frac{\\partial \\mathcal{L}}{\\partial A} V^\\top$\n",
    "\n",
    "2. Softmax (row-wise for each query $i$):\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial s_i} = J_{\\text{softmax}}(p_i)^{\\top}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p_i},\n",
    "\\quad\n",
    "J_{\\text{softmax}}(p_i) = \\mathrm{diag}(p_i) - p_i p_i^\\top\n",
    "$\n",
    "\n",
    "3. Through $S = QK^\\top / \\sqrt{d_k}$:\n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Q}\n",
    "= \\frac{1}{\\sqrt{d_k}} \\frac{\\partial \\mathcal{L}}{\\partial S} K,\n",
    "\\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial K}\n",
    "= \\frac{1}{\\sqrt{d_k}}\n",
    "\\left(\\frac{\\partial \\mathcal{L}}{\\partial S}\\right)^{\\top} Q\n",
    "$\n",
    "\n",
    "4. Through projections:\n",
    "   \n",
    "$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_Q} = X^\\top \\frac{\\partial \\mathcal{L}}{\\partial Q},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_K} = X^\\top \\frac{\\partial \\mathcal{L}}{\\partial K},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_V} = X^\\top \\frac{\\partial \\mathcal{L}}{\\partial V}\n",
    "$\n",
    "\n",
    "$ \\frac{\\partial \\mathcal{L}}{\\partial X} = \\frac{\\partial \\mathcal{L}}{\\partial Q} W_Q^\\top + \\frac{\\partial \\mathcal{L}}{\\partial K} W_K^\\top + \\frac{\\partial \\mathcal{L}}{\\partial V} W_V^\\top $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93d65f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from manual forward: 2.86e-02\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Forward without autograd\n",
    "with torch.no_grad():\n",
    "    Q = X @ W_Q\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    S = (Q @ K.t()) * scale\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    A = P @ V\n",
    "    L = 0.5 * torch.sum((A - T)**2)\n",
    "\n",
    "# 1) dL/dA\n",
    "dA = (A - T)\n",
    "\n",
    "# 2) Through A = P V\n",
    "dV = P.t() @ dA\n",
    "dP = dA @ V.t()\n",
    "\n",
    "# 3) Softmax Jacobian row-wise\n",
    "def softmax_grad_row(p_row, dp_row):\n",
    "    # p_row, dp_row: 1D tensors of length n\n",
    "    p = p_row.view(-1, 1)\n",
    "    J = torch.diagflat(p) - p @ p.t()\n",
    "    return (J.t() @ dp_row.view(-1, 1)).view(-1)\n",
    "\n",
    "n = P.shape[0]\n",
    "dS = torch.zeros_like(S)\n",
    "for i in range(n):\n",
    "    dS[i] = softmax_grad_row(P[i], dP[i])\n",
    "\n",
    "# 4) Through S = (QK^T)/sqrt(d_k)\n",
    "dQ = (dS @ K) * scale\n",
    "dK = (dS.t() @ Q) * scale\n",
    "\n",
    "# 5) Through projections\n",
    "dW_Q = X.t() @ dQ\n",
    "dW_K = X.t() @ dK\n",
    "dW_V = X.t() @ dV\n",
    "\n",
    "dX = dQ @ W_Q.t() + dK @ W_K.t() + dV @ W_V.t()\n",
    "\n",
    "grads_manual = {\n",
    "    \"X\": dX,\n",
    "    \"W_Q\": dW_Q,\n",
    "    \"W_K\": dW_K,\n",
    "    \"W_V\": dW_V,\n",
    "    \"Q\": dQ,\n",
    "    \"K\": dK,\n",
    "    \"V\": dV,\n",
    "    \"S\": dS,\n",
    "    \"P\": dP,\n",
    "    \"A\": dA,\n",
    "}\n",
    "\n",
    "print(f\"Loss from manual forward: {L:.{PRECISION}e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb1f6d",
   "metadata": {},
   "source": [
    "## 5. Compare Manual vs Autograd Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4512c1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max |manual - autograd| for W_Q: 7.28e-12\n",
      "Max |manual - autograd| for W_K: 2.91e-11\n",
      "Max |manual - autograd| for W_V: 0.00e+00\n",
      "Max |manual - autograd| for X: 1.86e-09\n",
      "\n",
      "Autograd dL/dW_Q:\n",
      " tensor([[-2.54e-04, -6.72e-05,  6.62e-05,  1.79e-04],\n",
      "        [ 1.59e-04,  3.44e-05, -6.72e-05, -9.17e-05],\n",
      "        [-1.86e-04, -4.14e-05,  7.43e-05,  1.11e-04],\n",
      "        [ 1.40e-04,  2.62e-05, -7.27e-05, -7.00e-05]])\n",
      "\n",
      "Manual   dL/dW_Q:\n",
      " tensor([[-2.54e-04, -6.72e-05,  6.62e-05,  1.79e-04],\n",
      "        [ 1.59e-04,  3.44e-05, -6.72e-05, -9.17e-05],\n",
      "        [-1.86e-04, -4.14e-05,  7.43e-05,  1.11e-04],\n",
      "        [ 1.40e-04,  2.62e-05, -7.27e-05, -7.00e-05]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def max_abs_diff(a, b):\n",
    "    return float(torch.max(torch.abs(a - b)))\n",
    "\n",
    "for name in [\"W_Q\", \"W_K\", \"W_V\", \"X\"]:\n",
    "    diff = max_abs_diff(grads_manual[name], grads_auto[name])\n",
    "    print(f\"Max |manual - autograd| for {name}: {diff:.{PRECISION}e}\")\n",
    "\n",
    "print(\"\\nAutograd dL/dW_Q:\\n\", grads_auto[\"W_Q\"])\n",
    "print(\"\\nManual   dL/dW_Q:\\n\", grads_manual[\"W_Q\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc11036",
   "metadata": {},
   "source": [
    "## 6. Finite-Difference Check\n",
    "\n",
    "Pick one element of $W_Q$ and approximate its gradient numerically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67387633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite-diff grad for W_Q[0,0]: -2.51e-04\n",
      "Autograd grad: -2.54e-04\n",
      "Manual grad: -2.54e-04\n"
     ]
    }
   ],
   "source": [
    "def compute_loss_with_WQ(WQ_new):\n",
    "    Q = X @ WQ_new\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "    S = (Q @ K.t()) * scale\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    A = P @ V\n",
    "    return 0.5 * torch.sum((A - T)**2)\n",
    "\n",
    "i, j = 0, 0  # test this entry\n",
    "eps = 1e-4\n",
    "\n",
    "with torch.no_grad():\n",
    "    WQ_plus = W_Q.clone();  WQ_plus[i, j] += eps\n",
    "    WQ_minus = W_Q.clone(); WQ_minus[i, j] -= eps\n",
    "    L_plus = compute_loss_with_WQ(WQ_plus)\n",
    "    L_minus = compute_loss_with_WQ(WQ_minus)\n",
    "    num_grad = (L_plus - L_minus) / (2 * eps)\n",
    "\n",
    "print(f\"Finite-diff grad for W_Q[{i},{j}]: {float(num_grad):.{PRECISION}e}\")\n",
    "print(f\"Autograd grad: {float(grads_auto['W_Q'][i,j]):.{PRECISION}e}\")\n",
    "print(f\"Manual grad: {float(grads_manual['W_Q'][i,j]):.{PRECISION}e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d166e8d",
   "metadata": {},
   "source": [
    "## 7. What to Notice\n",
    "\n",
    "- Manual gradients match autograd (up to numerical precision).\n",
    "- Each query position's loss sends signal to **all** keys/values it attends to.\n",
    "- The softmax Jacobian step is where this coupling happens.\n",
    "- The $1/\\sqrt{d_k}$ factor keeps scores and gradients in a stable range.\n",
    "\n",
    "## 8. Do the following\n",
    "- Change the target $T$ or matrices and re-run cells. Predict qualitatively how attention weights and gradients will shift.\n",
    "- Toggle scaling and compare graident norms, i.e. $S = \\frac{QK^\\top}{\\sqrt{d_k}}$ vs. $S = QK^\\top$\n",
    "- Plot heatmaps for $P$, $\\frac{\\partial {L}}{\\partial {P}}$, and $\\frac{\\partial {L}}{\\partial {P}}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
