{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RichardJPovinelli/Neural_Networks_Course/blob/main/train_char_rnn.ipynb)\n",
        "\n",
        "# Char-level RNN Training\n",
        "\n",
        "This notebook trains a small character-level RNN on the Tiny Shakespeare corpus (or any provided text file).\n",
        "\n",
        "Instructions:\n",
        "- In Google Colab, go to Runtime → Change runtime type → set Hardware accelerator = GPU for faster training.\n",
        "- Edit the Parameters cell (Cell 3) to adjust training constants (epochs, learning rate, sequence length, etc.).\n",
        "- Run the cells top-to-bottom. Checkpoints will be saved in a `checkpoints/` folder.\n",
        "- To resume, set `RESUME_PATH` to a checkpoint filename (e.g., `\"char_rnn_epoch10.pt\"`) or an absolute path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.9.0+cu128\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Imports and environment checks\n",
        "import os\n",
        "import types\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Parameters (edit these constants to control training and sampling)\n",
        "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "DATA_FILE = \"tiny_shakespeare.txt\"  # Set to a local file if you provide your own text\n",
        "\n",
        "# Training hyperparameters\n",
        "EPOCHS = 10\n",
        "LR = 1e-3\n",
        "SEQ_LEN = 100\n",
        "BATCH_SIZE = 32\n",
        "EMB_SIZE = 64\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 1\n",
        "RNN_TYPE = \"rnn\"  # one of: \"gru\", \"rnn\", \"lstm\"\n",
        "PRINT_EVERY = 200\n",
        "\n",
        "# Sampling\n",
        "START_TEXT = \"To be, or not to be\"\n",
        "TEMP = 1.0\n",
        "SAMPLE_LENGTH = 200\n",
        "\n",
        "# Checkpointing\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "RESUME_PATH = None  # e.g., \"char_rnn_epoch10.pt\" or an absolute path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Model definition \n",
        "class CharRNN(nn.Module):\n",
        "    \"\"\"A small char-level RNN for next-character prediction.\n",
        "\n",
        "    Architecture:\n",
        "    - Embedding-like linear projection of one-hot inputs\n",
        "    - RNN (nn.RNN, nn.GRU, or nn.LSTM)\n",
        "    - Linear decoder to vocab-size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size=32, hidden_size=128, num_layers=1, rnn_type=\"gru\"):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn_type = rnn_type.lower()\n",
        "\n",
        "        # Input projection: one-hot -> embedding-like linear\n",
        "        self.input_proj = nn.Linear(vocab_size, embedding_size)\n",
        "\n",
        "        if self.rnn_type == \"gru\":\n",
        "            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        elif self.rnn_type == \"rnn\":\n",
        "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        elif self.rnn_type == \"lstm\":\n",
        "            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown rnn_type\")\n",
        "\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x_onehot, hidden=None):\n",
        "        # x_onehot: (batch, seq_len, vocab_size)\n",
        "        emb = self.input_proj(x_onehot)\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        logits = self.decoder(out)\n",
        "        return logits, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1, device=None):\n",
        "        device = device or next(self.parameters()).device\n",
        "        if self.rnn_type == \"lstm\":\n",
        "            return (\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n",
        "            )\n",
        "        else:\n",
        "            return torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Helper functions (ported from train_char_rnn.py)\n",
        "def download_data(url=DATA_URL, path=DATA_FILE):\n",
        "    if os.path.exists(path):\n",
        "        return path\n",
        "    print(f\"Downloading {url} -> {path} ...\")\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(r.text)\n",
        "    return path\n",
        "\n",
        "def build_vocab(corpus_text):\n",
        "    chars = sorted(list(set(corpus_text)))\n",
        "    char_to_index = {ch: i for i, ch in enumerate(chars)}\n",
        "    index_to_char = {i: ch for ch, i in char_to_index.items()}\n",
        "    return chars, char_to_index, index_to_char\n",
        "\n",
        "def create_batches(corpus_text, char_to_index, seq_len, batch_size):\n",
        "    # Convert to indices\n",
        "    data = torch.tensor([char_to_index[ch] for ch in corpus_text], dtype=torch.long)\n",
        "    num_batches = data.size(0) // (batch_size * seq_len)\n",
        "    if num_batches == 0:\n",
        "        raise ValueError(\"Not enough data. Reduce seq_len or batch_size.\")\n",
        "    data = data[: num_batches * batch_size * seq_len]\n",
        "    data = data.view(batch_size, -1)\n",
        "    for i in range(0, data.size(1), seq_len):\n",
        "        x = data[:, i : i + seq_len]\n",
        "        y = data[:, i + 1 : i + 1 + seq_len]\n",
        "        if x.size(1) != seq_len or y.size(1) != seq_len:\n",
        "            continue\n",
        "        yield x, y\n",
        "\n",
        "def one_hot(indices, vocab_size, device):\n",
        "    # indices: (batch, seq_len) long\n",
        "    b, s = indices.size()\n",
        "    oh = torch.zeros(b, s, vocab_size, device=device)\n",
        "    oh.scatter_(2, indices.unsqueeze(-1), 1.0)\n",
        "    return oh\n",
        "\n",
        "def sample(model, start_str, char_to_index, index_to_char, length=200, temperature=1.0, device=None):\n",
        "    model.eval()\n",
        "    vocab_size = len(char_to_index)\n",
        "    device = device or torch.device(\"cpu\")\n",
        "    if not isinstance(device, torch.device):\n",
        "        device = torch.device(device)\n",
        "    hidden = model.init_hidden(batch_size=1, device=device)\n",
        "\n",
        "    input_idx = torch.tensor([[char_to_index[ch] for ch in start_str]], dtype=torch.long, device=device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(input_idx.size(1)):\n",
        "            x_oh = one_hot(input_idx[:, i : i + 1], vocab_size, device)\n",
        "            logits, hidden = model(x_oh, hidden)\n",
        "        out_chars = list(start_str)\n",
        "        prev = input_idx[:, -1:]\n",
        "        for _ in range(length):\n",
        "            x_oh = one_hot(prev, vocab_size, device)\n",
        "            logits, hidden = model(x_oh, hidden)\n",
        "            logits = logits[:, -1, :] / max(1e-8, temperature)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx = torch.multinomial(probs, num_samples=1)\n",
        "            idx_item = int(idx.view(-1).cpu().numpy()[0])\n",
        "            ch = index_to_char[idx_item]\n",
        "            out_chars.append(ch)\n",
        "            prev = idx\n",
        "    return \"\".join(out_chars)\n",
        "\n",
        "def train(config):\n",
        "    preferred_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    path = download_data()\n",
        "    corpus_text = open(path, encoding=\"utf-8\").read()\n",
        "    chars, char_to_index, index_to_char = build_vocab(corpus_text)\n",
        "    vocab_size = len(chars)\n",
        "    print(f\"Loaded data: {len(corpus_text)} chars, {vocab_size} unique chars\")\n",
        "\n",
        "    model = CharRNN(\n",
        "        vocab_size,\n",
        "        embedding_size=config.emb_size,\n",
        "        hidden_size=config.hidden,\n",
        "        num_layers=config.layers,\n",
        "        rnn_type=config.rnn,\n",
        "    )\n",
        "    try:\n",
        "        model.to(preferred_device)\n",
        "        dev = preferred_device\n",
        "        print(f\"Using device: {dev}\")\n",
        "    except (RuntimeError, OSError) as e:\n",
        "        dev = torch.device(\"cpu\")\n",
        "        model.to(dev)\n",
        "        print(f\"Warning: failed to use preferred device {preferred_device} ({e}), falling back to CPU\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    ckpt_dir = os.path.join(os.getcwd(), \"checkpoints\")\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "    start_epoch = 1\n",
        "    if config.resume:\n",
        "        resume_path = config.resume\n",
        "        if not os.path.isabs(resume_path):\n",
        "            resume_path = os.path.join(ckpt_dir, resume_path)\n",
        "        if not os.path.exists(resume_path):\n",
        "            alt_path = os.path.join(os.getcwd(), config.resume)\n",
        "            if os.path.exists(alt_path):\n",
        "                resume_path = alt_path\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"Checkpoint to resume not found: {resume_path}\")\n",
        "        print(f\"Loading checkpoint from {resume_path}\")\n",
        "        ckpt = torch.load(resume_path, map_location=dev)\n",
        "        model.load_state_dict(ckpt[\"model_state\"])\n",
        "        if \"optimizer_state\" in ckpt:\n",
        "            try:\n",
        "                optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "            except (RuntimeError, ValueError):\n",
        "                print(\"Warning: Could not load optimizer state; continuing with fresh optimizer\")\n",
        "        if \"char_to_index\" in ckpt and \"index_to_char\" in ckpt:\n",
        "            char_to_index = ckpt[\"char_to_index\"]\n",
        "            index_to_char = ckpt[\"index_to_char\"]\n",
        "            chars = list(index_to_char.values())\n",
        "            vocab_size = len(chars)\n",
        "        if \"epoch\" in ckpt:\n",
        "            start_epoch = ckpt[\"epoch\"] + 1\n",
        "        print(f\"Resuming from epoch {start_epoch}\")\n",
        "\n",
        "    steps = 0\n",
        "    for epoch in range(start_epoch, config.epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        batches = 0\n",
        "\n",
        "        for x_batch, y_batch in create_batches(corpus_text, char_to_index, config.seq_len, config.batch_size):\n",
        "            batches += 1\n",
        "            x = one_hot(x_batch.to(dev), vocab_size, dev)\n",
        "            y = y_batch.to(dev)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = model(x)\n",
        "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += float(loss)\n",
        "            steps += 1\n",
        "            if steps % config.print_every == 0:\n",
        "                print(f\"Epoch {epoch} step {steps} loss {total_loss / steps:.4f}\")\n",
        "\n",
        "        avg = total_loss / max(1, batches)\n",
        "        print(f\"Epoch {epoch} average loss {avg:.4f}\")\n",
        "\n",
        "        s = sample(\n",
        "            model,\n",
        "            start_str=config.start,\n",
        "            char_to_index=char_to_index,\n",
        "            index_to_char=index_to_char,\n",
        "            length=200,\n",
        "            temperature=config.temp,\n",
        "            device=dev,\n",
        "        )\n",
        "        print(f\"\\nSample:\\n{'-' * 100}\\n{s[:1000]}\\n{'-' * 100}\\n\")\n",
        "\n",
        "        ckpt_name = f\"char_rnn_epoch{epoch}.pt\"\n",
        "        ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"vocab\": chars,\n",
        "                \"char_to_index\": char_to_index,\n",
        "                \"index_to_char\": index_to_char,\n",
        "            },\n",
        "            ckpt_path,\n",
        "        )\n",
        "        print(f\"Saved checkpoint {ckpt_path}\")\n",
        "\n",
        "    return model, chars, char_to_index, index_to_char, dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded data: 1115394 chars, 65 unique chars\n",
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\richard\\AppData\\Local\\Temp\\ipykernel_52472\\2710364178.py:143: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
            "  total_loss += float(loss)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 step 200 loss 2.8811\n",
            "Epoch 1 average loss 2.6235\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to be; chedl of the fipren;\n",
            "Loml thiure pangsezef wheat, for darls.\n",
            "Rundt ir laad'r wish cemysing comtunceror'e un-seete;\n",
            "Shan whe angilu, sham bamd whow jut at alghr all willther\n",
            "I hos\n",
            "Apre: ann eneat' ab\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch1.pt\n",
            "Epoch 2 step 400 loss 0.2860\n",
            "Epoch 2 step 600 loss 0.8763\n",
            "Epoch 2 average loss 2.0479\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to ben pay, so futicewion macgoo sored;\n",
            "Ses, is brootherst?\n",
            "Mure thou kend you likswer.\n",
            "\n",
            "VOZABESHO:\n",
            "Bain dave ther Ifge raght meastsry? pay bus I mupg never the iare!\n",
            "What Gift, thou dut mare how\n",
            "WoY shaig\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch2.pt\n",
            "Epoch 3 step 800 loss 0.2535\n",
            "Epoch 3 step 1000 loss 0.5728\n",
            "Epoch 3 average loss 1.8657\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to beam these pentulinout;\n",
            "O your that some mugit,\n",
            "bey a vaith at is s\n",
            "Molist sire rugn; there ame your yours.\n",
            "\n",
            "DIZANTES:\n",
            "Thou show's was beong thou uxery shall.\n",
            "\n",
            "PRIONEXIS:\n",
            "Set, so, and your wife and 'Tis\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch3.pt\n",
            "Epoch 4 step 1200 loss 0.2359\n",
            "Epoch 4 average loss 1.7586\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to bens meer worswer cain't fauges pliadery worth stafe is hame weltoled, Thouth servener by tert, will-metet of manie, I dives you foren why lood our reave mest here serreds,\n",
            "thereagher wordul sister\n",
            "Your\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch4.pt\n",
            "Epoch 5 step 1400 loss 0.0148\n",
            "Epoch 5 step 1600 loss 0.2250\n",
            "Epoch 5 average loss 1.6882\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to ben swert ere leet you she slaws of such a mistrest,\n",
            "Onkin: sir lights the briver werd news ray to state thee, gender thought!\n",
            "Will the king:\n",
            "my word to the gaintion kits\n",
            "With hear queat was atitor trag\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch5.pt\n",
            "Epoch 6 step 1800 loss 0.0602\n",
            "Epoch 6 step 2000 loss 0.2176\n",
            "Epoch 6 average loss 1.6386\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to ben a shret;\n",
            "He's month fas that you heary I counds is noble's day\n",
            "Doighter menstion\n",
            "Is tilotron of the nom word to you, like, what is my warw\n",
            "not mernerous wart, for your lint heavant nor\n",
            "Thinefulle, a\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch6.pt\n",
            "Epoch 7 step 2200 loss 0.0866\n",
            "Epoch 7 step 2400 loss 0.2121\n",
            "Epoch 7 average loss 1.6013\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to ber thou art;\n",
            "I child, do rusted me, well, to jet a traniens, and sweet these fallers.\n",
            "Have own Edwast I too wor danging of wer nepes: but one upcuen awakent the lazom;\n",
            "bain thouse brother canist cower.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch7.pt\n",
            "Epoch 8 step 2600 loss 0.1039\n",
            "Epoch 8 average loss 1.5723\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to ben ran Henrless,\n",
            "And kear thou; thou know the tongues, I caunts all parted my exter.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I you me stittle arventer soul,\n",
            "To gentle veave time\n",
            "which had press\n",
            "She ore hatm us risher frant t\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch8.pt\n",
            "Epoch 9 step 2800 loss 0.0134\n",
            "Epoch 9 step 3000 loss 0.1160\n",
            "Epoch 9 average loss 1.5492\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to ben,\n",
            "And thine is beghard,\n",
            "And fires trwet; must short men to rest his fien; there; but the inforted you sosted of my fair are wingen to that is sea, field?\n",
            "\n",
            "PORDONS:\n",
            "How no, all fellow\n",
            "And weir boute.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch9.pt\n",
            "Epoch 10 step 3200 loss 0.0372\n",
            "Epoch 10 step 3400 loss 0.1247\n",
            "Epoch 10 average loss 1.5304\n",
            "\n",
            "Sample:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "To be, or not to bet of my face.\n",
            "\n",
            "CORIOLANUS:\n",
            "You let's up the land, as Ih benounceed,\n",
            "But you that? 'lord'd: O tet I lamer stoal, one\n",
            "King where we disword; and sweet pales a good mornvecs, sty, to me the Duke and cast\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Saved checkpoint c:\\Users\\richard\\mu\\teaching\\Neural Networks\\lectures\\06 Recurent Networks\\code\\checkpoints\\char_rnn_epoch10.pt\n"
          ]
        }
      ],
      "source": [
        "# Launch training with the parameters above\n",
        "config = types.SimpleNamespace(\n",
        "    epochs=EPOCHS,\n",
        "    lr=LR,\n",
        "    seq_len=SEQ_LEN,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    emb_size=EMB_SIZE,\n",
        "    hidden=HIDDEN_SIZE,\n",
        "    layers=NUM_LAYERS,\n",
        "    rnn=RNN_TYPE,\n",
        "    print_every=PRINT_EVERY,\n",
        "    start=START_TEXT,\n",
        "    temp=TEMP,\n",
        "    resume=RESUME_PATH,\n",
        ")\n",
        "\n",
        "model, chars, char_to_index, index_to_char, dev = train(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Generation ---\n",
            "\n",
            "To be, or not to ben of tull I child, homal herewnets theur strokeail, of you, come agains\n",
            "Ro loys;\n",
            "Stay were to thence again; I'll no man\n",
            "Can I voot, earther and pility knees, I am sit. O, the mother.\n",
            "\n",
            "SLARDIBA:\n",
            "I make\n"
          ]
        }
      ],
      "source": [
        "# Generate a sample after training (edit START_TEXT, SAMPLE_LENGTH, TEMP in the Parameters cell)\n",
        "print(\"\\n--- Generation ---\\n\")\n",
        "txt = sample(\n",
        "    model,\n",
        "    start_str=START_TEXT,\n",
        "    char_to_index=char_to_index,\n",
        "    index_to_char=index_to_char,\n",
        "    length=SAMPLE_LENGTH,\n",
        "    temperature=TEMP,\n",
        "    device=dev,\n",
        ")\n",
        "print(txt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
